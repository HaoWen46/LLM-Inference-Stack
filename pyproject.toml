[project]
name = "llm-inference-stack"
version = "0.2.0"
description = "Production LLM inference stack (Rust gateway + Python load tester)"
requires-python = ">=3.10"
dependencies = [
    # vLLM (kept as-is â€” requires GPU + CUDA)
    "vllm==0.6.6",
    # Load tester
    "httpx[http2]==0.28.1",
    "rich==13.9.4",
    "typer==0.15.1",
    "numpy==1.26.4",
    # Monitoring / scripts
    "prometheus-client==0.21.1",
    "structlog==24.4.0",
    "python-dotenv==1.0.1",
    "pydantic==2.10.3",
    "pydantic-settings==2.7.0",
    "aiofiles==24.1.0",
]

[project.scripts]
# Run the load tester via: uv run loadtest
loadtest = "loadtest.runner:main"

[tool.uv]
dev-dependencies = [
    "pytest>=8.0",
    "pytest-asyncio>=0.24",
    "httpx>=0.28",
]

[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"
