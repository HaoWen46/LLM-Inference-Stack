# Copy to .env and fill in values

# Model
MODEL_NAME=meta-llama/Llama-3.2-7B-Instruct
MODEL_CACHE_DIR=/home5/B11902156/LLM/models
SERVED_MODEL_NAME=llama-7b

# GPU selection — explicit indices to avoid surprises with mixed GPU fleets
CUDA_VISIBLE_DEVICES=0,1
CUDA_DEVICE_ORDER=PCI_BUS_ID

# Parallelism
TP_SIZE=1
PP_SIZE=1

# Memory
GPU_MEMORY_UTILIZATION=0.90
MAX_MODEL_LEN=8192
MAX_NUM_SEQS=256
MAX_NUM_BATCHED_TOKENS=8192
SWAP_SPACE_GB=8

# Quantization: none | awq | gptq | bitsandbytes
QUANTIZATION=none
DTYPE=bfloat16

# vLLM server
VLLM_HOST=0.0.0.0
VLLM_PORT=8000
VLLM_API_KEY=vllm-secret-change-me

# Gateway
GATEWAY_HOST=0.0.0.0
GATEWAY_PORT=8080
GATEWAY_API_KEYS=dev-key-1,dev-key-2
RATE_LIMIT_PER_MINUTE=60
REQUEST_TIMEOUT_SECONDS=300

# Observability
LOG_LEVEL=INFO
PROMETHEUS_PORT=9090
GRAFANA_PORT=3000

# Cache dirs — redirect away from home partition if quota is tight
# VLLM_CACHE_ROOT=/fast-disk/.cache/vllm

# HuggingFace
HF_TOKEN=
